{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "288446e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9156fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/jovyan/work/input/titanic/train.csv\")\n",
    "test = pd.read_csv(\"/home/jovyan/work/input/titanic/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f570a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train.drop(['Survived'], axis=1)\n",
    "train_y = train['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62f68cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17b5d575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>3</td>\n",
       "      <td>Kelly, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330911</td>\n",
       "      <td>7.8292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
       "      <td>female</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>363272</td>\n",
       "      <td>7.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>2</td>\n",
       "      <td>Myles, Mr. Thomas Francis</td>\n",
       "      <td>male</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>240276</td>\n",
       "      <td>9.6875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>3</td>\n",
       "      <td>Wirz, Mr. Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>315154</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>3</td>\n",
       "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
       "      <td>female</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3101298</td>\n",
       "      <td>12.2875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>3</td>\n",
       "      <td>Spector, Mr. Woolf</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>A.5. 3236</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "      <td>Oliva y Ocana, Dona. Fermina</td>\n",
       "      <td>female</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17758</td>\n",
       "      <td>108.9000</td>\n",
       "      <td>C105</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>3</td>\n",
       "      <td>Saether, Mr. Simon Sivertsen</td>\n",
       "      <td>male</td>\n",
       "      <td>38.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>SOTON/O.Q. 3101262</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>3</td>\n",
       "      <td>Ware, Mr. Frederick</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>359309</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>3</td>\n",
       "      <td>Peter, Master. Michael J</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2668</td>\n",
       "      <td>22.3583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                                          Name  \\\n",
       "0            892       3                              Kelly, Mr. James   \n",
       "1            893       3              Wilkes, Mrs. James (Ellen Needs)   \n",
       "2            894       2                     Myles, Mr. Thomas Francis   \n",
       "3            895       3                              Wirz, Mr. Albert   \n",
       "4            896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)   \n",
       "..           ...     ...                                           ...   \n",
       "413         1305       3                            Spector, Mr. Woolf   \n",
       "414         1306       1                  Oliva y Ocana, Dona. Fermina   \n",
       "415         1307       3                  Saether, Mr. Simon Sivertsen   \n",
       "416         1308       3                           Ware, Mr. Frederick   \n",
       "417         1309       3                      Peter, Master. Michael J   \n",
       "\n",
       "        Sex   Age  SibSp  Parch              Ticket      Fare Cabin Embarked  \n",
       "0      male  34.5      0      0              330911    7.8292   NaN        Q  \n",
       "1    female  47.0      1      0              363272    7.0000   NaN        S  \n",
       "2      male  62.0      0      0              240276    9.6875   NaN        Q  \n",
       "3      male  27.0      0      0              315154    8.6625   NaN        S  \n",
       "4    female  22.0      1      1             3101298   12.2875   NaN        S  \n",
       "..      ...   ...    ...    ...                 ...       ...   ...      ...  \n",
       "413    male   NaN      0      0           A.5. 3236    8.0500   NaN        S  \n",
       "414  female  39.0      0      0            PC 17758  108.9000  C105        C  \n",
       "415    male  38.5      0      0  SOTON/O.Q. 3101262    7.2500   NaN        S  \n",
       "416    male   NaN      0      0              359309    8.0500   NaN        S  \n",
       "417    male   NaN      1      1                2668   22.3583   NaN        C  \n",
       "\n",
       "[418 rows x 11 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "832080a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "train_x = train_x.drop(['PassengerId'], axis = 1)\n",
    "test_x = test_x.drop(['PassengerId'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a2c0636",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.drop(['Name', 'Ticket', 'Cabin'], axis = 1)\n",
    "test_x = test_x.drop(['Name', 'Ticket', 'Cabin'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "387b0f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sex\n",
      "Embarked\n"
     ]
    }
   ],
   "source": [
    "for c in ['Sex', 'Embarked']:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_x[c].fillna('NA'))\n",
    "    \n",
    "    train_x[c] = le.transform(train_x[c].fillna('NA'))\n",
    "    test_x[c] = le.transform(test_x[c].fillna('NA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21be3e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex   Age  SibSp  Parch     Fare  Embarked\n",
       "0         3    1  22.0      1      0   7.2500         3\n",
       "1         1    0  38.0      1      0  71.2833         0\n",
       "2         3    0  26.0      0      0   7.9250         3\n",
       "3         1    0  35.0      1      0  53.1000         3\n",
       "4         3    1  35.0      0      0   8.0500         3\n",
       "..      ...  ...   ...    ...    ...      ...       ...\n",
       "886       2    1  27.0      0      0  13.0000         3\n",
       "887       1    0  19.0      0      0  30.0000         3\n",
       "888       3    0   NaN      1      2  23.4500         3\n",
       "889       1    1  26.0      0      0  30.0000         0\n",
       "890       3    1  32.0      0      0   7.7500         2\n",
       "\n",
       "[891 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b33b19aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:51:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=20, n_jobs=2,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=71,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(n_estimators=20, random_state=71)\n",
    "model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31c36a49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04495935 0.08867919 0.17907852 0.1694976  0.4144685  0.09883825\n",
      " 0.295474   0.09249494 0.780758   0.0334494  0.05271804 0.12275251\n",
      " 0.98265195 0.10036837 0.9783865  0.8869843  0.09802166 0.34121916\n",
      " 0.4545685  0.2103832  0.38970888 0.2503306  0.98956656 0.5478257\n",
      " 0.8888248  0.06096522 0.99501413 0.23129131 0.54666406 0.3449203\n",
      " 0.06096522 0.06886324 0.8451166  0.18461095 0.700836   0.23888698\n",
      " 0.19918925 0.23014943 0.07286744 0.5179445  0.08917417 0.48201504\n",
      " 0.05414041 0.9396665  0.9752612  0.16563176 0.26199064 0.10033685\n",
      " 0.98084486 0.84173465 0.28447428 0.2153398  0.93277276 0.9246002\n",
      " 0.43748927 0.04151895 0.04164062 0.09208392 0.13564435 0.98407745\n",
      " 0.07393947 0.31570944 0.09847555 0.7108621  0.580982   0.97924876\n",
      " 0.66925937 0.08677519 0.36354595 0.88125867 0.46547    0.0450958\n",
      " 0.43701395 0.471791   0.96612316 0.47401097 0.11102423 0.86922896\n",
      " 0.10644802 0.46547    0.94631976 0.21509618 0.21027103 0.05271804\n",
      " 0.1153224  0.24954471 0.5610257  0.39401576 0.81795925 0.96278036\n",
      " 0.3139115  0.10033685 0.8999428  0.11102423 0.7545185  0.16894393\n",
      " 0.97407687 0.2812703  0.62517625 0.09745738 0.98522866 0.10721484\n",
      " 0.10033685 0.203665   0.50727224 0.06356314 0.06915081 0.10033685\n",
      " 0.1153224  0.06207458 0.08917417 0.81795925 0.9905811  0.7790363\n",
      " 0.9678478  0.22985835 0.03328842 0.83971477 0.5751565  0.9400439\n",
      " 0.9524556  0.10033685 0.9913566  0.08177242 0.10033685 0.49979553\n",
      " 0.10264422 0.8931861  0.11671428 0.08800445 0.23675226 0.12139394\n",
      " 0.19983698 0.03328842 0.05017959 0.08223343 0.44488266 0.07670616\n",
      " 0.29576984 0.03920994 0.0983394  0.97028905 0.403194   0.09279539\n",
      " 0.67225116 0.10715791 0.12769449 0.08800445 0.48201504 0.09825031\n",
      " 0.9905227  0.11576053 0.02282061 0.87007475 0.04454902 0.11811791\n",
      " 0.9700389  0.49685338 0.67225116 0.7620462  0.81795925 0.85801774\n",
      " 0.6764928  0.01125165 0.11671428 0.17766903 0.23219322 0.07110976\n",
      " 0.9722853  0.30704173 0.03139965 0.44488266 0.04259211 0.08933179\n",
      " 0.05206819 0.9699793  0.98767984 0.49617484 0.983521   0.9311823\n",
      " 0.10644802 0.5877444  0.98439807 0.10033685 0.826518   0.07135862\n",
      " 0.95509195 0.21644714 0.04357384 0.11671428 0.08754148 0.21325669\n",
      " 0.47493997 0.13105467 0.8318024  0.04654983 0.9667686  0.58485866\n",
      " 0.04545608 0.19918925 0.8683232  0.9705729  0.35474896 0.9507952\n",
      " 0.07286744 0.4065924  0.5092199  0.04545608 0.97374403 0.05676446\n",
      " 0.31570944 0.01125165 0.10375109 0.8488676  0.22717884 0.21931598\n",
      " 0.85902715 0.2083079  0.9471441  0.11102423 0.91714    0.08800445\n",
      " 0.9242309  0.10264422 0.7735132  0.25243038 0.10264422 0.81795925\n",
      " 0.07135862 0.09802166 0.18207763 0.96627676 0.06008311 0.05271804\n",
      " 0.5854071  0.07503933 0.26111168 0.3612836  0.94454587 0.97936857\n",
      " 0.7735132  0.92935324 0.45828494 0.05271804 0.07017034 0.30680296\n",
      " 0.94221604 0.08527933 0.9400439  0.38865012 0.9389023  0.2324003\n",
      " 0.35504788 0.04545608 0.10129018 0.03139965 0.10033685 0.09504994\n",
      " 0.919324   0.10264422 0.07135862 0.06915081 0.96620274 0.8155708\n",
      " 0.14216608 0.05271804 0.01681589 0.03139965 0.19918925 0.09883825\n",
      " 0.23262602 0.10033685 0.90147066 0.85669565 0.08933179 0.9549344\n",
      " 0.10592391 0.07135862 0.0856787  0.04545608 0.23014943 0.9705729\n",
      " 0.81795925 0.31175274 0.850009   0.01768319 0.03139965 0.20147325\n",
      " 0.08933179 0.11102423 0.12769449 0.26690945 0.08933179 0.17875293\n",
      " 0.09802166 0.07038315 0.9453739  0.3449203  0.2098845  0.22493187\n",
      " 0.17733616 0.22641651 0.0726205  0.04545608 0.81795925 0.85679317\n",
      " 0.39342153 0.9746968  0.2707192  0.12674552 0.09883825 0.34121916\n",
      " 0.03139965 0.28198424 0.97657984 0.64283156 0.33916283 0.09883825\n",
      " 0.24882805 0.06886324 0.203665   0.20202988 0.07670616 0.7869925\n",
      " 0.96317667 0.07102375 0.96405846 0.23262602 0.09825031 0.04259211\n",
      " 0.9588245  0.39089742 0.08933179 0.4591689  0.17395009 0.28040078\n",
      " 0.31570944 0.03782169 0.13562514 0.08933179 0.09883825 0.07905845\n",
      " 0.06850064 0.9698038  0.08837242 0.62004155 0.07670616 0.36507472\n",
      " 0.04259211 0.9772749  0.9848011  0.07286744 0.10375109 0.08523976\n",
      " 0.850009   0.15499827 0.97204936 0.05271804 0.10033685 0.38814533\n",
      " 0.02595753 0.98952067 0.9772749  0.1694976  0.9628237  0.16147791\n",
      " 0.24954471 0.1743162  0.9848011  0.20978004 0.04545608 0.99303657\n",
      " 0.03134613 0.11671428 0.9730215  0.97808456 0.12891217 0.04545608\n",
      " 0.08938846 0.07437627 0.10033685 0.07038315 0.4128122  0.2996218\n",
      " 0.13564435 0.97904176 0.16136314 0.10042924 0.10264422 0.04813124\n",
      " 0.36330256 0.972909   0.13676274 0.09529299 0.03622868 0.9889205\n",
      " 0.11811791 0.9834789  0.10264422 0.13470806 0.9618183  0.07135862\n",
      " 0.99501413 0.14415234 0.26475704 0.37816477 0.04545608 0.41643125\n",
      " 0.74728876 0.8257353  0.81795925 0.9896197  0.30061394 0.11102423\n",
      " 0.9916985  0.02141916 0.11102423 0.40364406]\n",
      "[[0.95504063 0.04495935]\n",
      " [0.9113208  0.08867919]\n",
      " [0.8209215  0.17907852]\n",
      " [0.8305024  0.1694976 ]\n",
      " [0.5855315  0.4144685 ]\n",
      " [0.90116173 0.09883825]\n",
      " [0.704526   0.295474  ]\n",
      " [0.90750504 0.09249494]\n",
      " [0.21924198 0.780758  ]\n",
      " [0.9665506  0.0334494 ]\n",
      " [0.94728196 0.05271804]\n",
      " [0.8772475  0.12275251]\n",
      " [0.01734805 0.98265195]\n",
      " [0.8996316  0.10036837]\n",
      " [0.02161348 0.9783865 ]\n",
      " [0.11301571 0.8869843 ]\n",
      " [0.9019784  0.09802166]\n",
      " [0.6587808  0.34121916]\n",
      " [0.5454315  0.4545685 ]\n",
      " [0.7896168  0.2103832 ]\n",
      " [0.6102911  0.38970888]\n",
      " [0.74966943 0.2503306 ]\n",
      " [0.01043344 0.98956656]\n",
      " [0.4521743  0.5478257 ]\n",
      " [0.11117518 0.8888248 ]\n",
      " [0.93903476 0.06096522]\n",
      " [0.00498587 0.99501413]\n",
      " [0.7687087  0.23129131]\n",
      " [0.45333594 0.54666406]\n",
      " [0.6550797  0.3449203 ]\n",
      " [0.93903476 0.06096522]\n",
      " [0.9311367  0.06886324]\n",
      " [0.15488338 0.8451166 ]\n",
      " [0.81538904 0.18461095]\n",
      " [0.299164   0.700836  ]\n",
      " [0.76111305 0.23888698]\n",
      " [0.80081075 0.19918925]\n",
      " [0.76985055 0.23014943]\n",
      " [0.92713255 0.07286744]\n",
      " [0.4820555  0.5179445 ]\n",
      " [0.91082585 0.08917417]\n",
      " [0.517985   0.48201504]\n",
      " [0.9458596  0.05414041]\n",
      " [0.06033349 0.9396665 ]\n",
      " [0.02473879 0.9752612 ]\n",
      " [0.8343682  0.16563176]\n",
      " [0.73800933 0.26199064]\n",
      " [0.89966315 0.10033685]\n",
      " [0.01915514 0.98084486]\n",
      " [0.15826535 0.84173465]\n",
      " [0.71552575 0.28447428]\n",
      " [0.7846602  0.2153398 ]\n",
      " [0.06722724 0.93277276]\n",
      " [0.07539982 0.9246002 ]\n",
      " [0.5625107  0.43748927]\n",
      " [0.9584811  0.04151895]\n",
      " [0.95835936 0.04164062]\n",
      " [0.90791607 0.09208392]\n",
      " [0.8643557  0.13564435]\n",
      " [0.01592255 0.98407745]\n",
      " [0.92606056 0.07393947]\n",
      " [0.6842905  0.31570944]\n",
      " [0.9015244  0.09847555]\n",
      " [0.2891379  0.7108621 ]\n",
      " [0.41901797 0.580982  ]\n",
      " [0.02075124 0.97924876]\n",
      " [0.33074063 0.66925937]\n",
      " [0.9132248  0.08677519]\n",
      " [0.63645405 0.36354595]\n",
      " [0.11874133 0.88125867]\n",
      " [0.53453004 0.46547   ]\n",
      " [0.9549042  0.0450958 ]\n",
      " [0.562986   0.43701395]\n",
      " [0.528209   0.471791  ]\n",
      " [0.03387684 0.96612316]\n",
      " [0.52598906 0.47401097]\n",
      " [0.88897574 0.11102423]\n",
      " [0.13077104 0.86922896]\n",
      " [0.893552   0.10644802]\n",
      " [0.53453004 0.46547   ]\n",
      " [0.05368024 0.94631976]\n",
      " [0.7849038  0.21509618]\n",
      " [0.789729   0.21027103]\n",
      " [0.94728196 0.05271804]\n",
      " [0.8846776  0.1153224 ]\n",
      " [0.75045526 0.24954471]\n",
      " [0.43897432 0.5610257 ]\n",
      " [0.6059842  0.39401576]\n",
      " [0.18204075 0.81795925]\n",
      " [0.03721964 0.96278036]\n",
      " [0.6860885  0.3139115 ]\n",
      " [0.89966315 0.10033685]\n",
      " [0.10005718 0.8999428 ]\n",
      " [0.88897574 0.11102423]\n",
      " [0.24548149 0.7545185 ]\n",
      " [0.83105606 0.16894393]\n",
      " [0.02592313 0.97407687]\n",
      " [0.71872973 0.2812703 ]\n",
      " [0.37482375 0.62517625]\n",
      " [0.9025426  0.09745738]\n",
      " [0.01477134 0.98522866]\n",
      " [0.8927852  0.10721484]\n",
      " [0.89966315 0.10033685]\n",
      " [0.796335   0.203665  ]\n",
      " [0.49272776 0.50727224]\n",
      " [0.9364369  0.06356314]\n",
      " [0.9308492  0.06915081]\n",
      " [0.89966315 0.10033685]\n",
      " [0.8846776  0.1153224 ]\n",
      " [0.9379254  0.06207458]\n",
      " [0.91082585 0.08917417]\n",
      " [0.18204075 0.81795925]\n",
      " [0.0094189  0.9905811 ]\n",
      " [0.22096372 0.7790363 ]\n",
      " [0.03215218 0.9678478 ]\n",
      " [0.77014166 0.22985835]\n",
      " [0.9667116  0.03328842]\n",
      " [0.16028523 0.83971477]\n",
      " [0.4248435  0.5751565 ]\n",
      " [0.05995607 0.9400439 ]\n",
      " [0.04754442 0.9524556 ]\n",
      " [0.89966315 0.10033685]\n",
      " [0.00864339 0.9913566 ]\n",
      " [0.91822755 0.08177242]\n",
      " [0.89966315 0.10033685]\n",
      " [0.50020444 0.49979553]\n",
      " [0.8973558  0.10264422]\n",
      " [0.10681391 0.8931861 ]\n",
      " [0.8832857  0.11671428]\n",
      " [0.91199553 0.08800445]\n",
      " [0.7632477  0.23675226]\n",
      " [0.8786061  0.12139394]\n",
      " [0.80016303 0.19983698]\n",
      " [0.9667116  0.03328842]\n",
      " [0.9498204  0.05017959]\n",
      " [0.9177666  0.08223343]\n",
      " [0.55511737 0.44488266]\n",
      " [0.9232938  0.07670616]\n",
      " [0.7042302  0.29576984]\n",
      " [0.96079004 0.03920994]\n",
      " [0.9016606  0.0983394 ]\n",
      " [0.02971095 0.97028905]\n",
      " [0.596806   0.403194  ]\n",
      " [0.9072046  0.09279539]\n",
      " [0.32774884 0.67225116]\n",
      " [0.8928421  0.10715791]\n",
      " [0.8723055  0.12769449]\n",
      " [0.91199553 0.08800445]\n",
      " [0.517985   0.48201504]\n",
      " [0.9017497  0.09825031]\n",
      " [0.00947732 0.9905227 ]\n",
      " [0.8842395  0.11576053]\n",
      " [0.9771794  0.02282061]\n",
      " [0.12992525 0.87007475]\n",
      " [0.95545095 0.04454902]\n",
      " [0.8818821  0.11811791]\n",
      " [0.02996111 0.9700389 ]\n",
      " [0.50314665 0.49685338]\n",
      " [0.32774884 0.67225116]\n",
      " [0.23795378 0.7620462 ]\n",
      " [0.18204075 0.81795925]\n",
      " [0.14198226 0.85801774]\n",
      " [0.3235072  0.6764928 ]\n",
      " [0.9887484  0.01125165]\n",
      " [0.8832857  0.11671428]\n",
      " [0.82233095 0.17766903]\n",
      " [0.76780677 0.23219322]\n",
      " [0.9288902  0.07110976]\n",
      " [0.02771473 0.9722853 ]\n",
      " [0.69295824 0.30704173]\n",
      " [0.96860033 0.03139965]\n",
      " [0.55511737 0.44488266]\n",
      " [0.9574079  0.04259211]\n",
      " [0.9106682  0.08933179]\n",
      " [0.9479318  0.05206819]\n",
      " [0.03002071 0.9699793 ]\n",
      " [0.01232016 0.98767984]\n",
      " [0.5038252  0.49617484]\n",
      " [0.01647902 0.983521  ]\n",
      " [0.06881768 0.9311823 ]\n",
      " [0.893552   0.10644802]\n",
      " [0.4122556  0.5877444 ]\n",
      " [0.01560193 0.98439807]\n",
      " [0.89966315 0.10033685]\n",
      " [0.173482   0.826518  ]\n",
      " [0.9286414  0.07135862]\n",
      " [0.04490805 0.95509195]\n",
      " [0.7835529  0.21644714]\n",
      " [0.95642614 0.04357384]\n",
      " [0.8832857  0.11671428]\n",
      " [0.91245854 0.08754148]\n",
      " [0.7867433  0.21325669]\n",
      " [0.52506006 0.47493997]\n",
      " [0.86894536 0.13105467]\n",
      " [0.16819757 0.8318024 ]\n",
      " [0.9534502  0.04654983]\n",
      " [0.03323138 0.9667686 ]\n",
      " [0.41514134 0.58485866]\n",
      " [0.95454395 0.04545608]\n",
      " [0.80081075 0.19918925]\n",
      " [0.1316768  0.8683232 ]\n",
      " [0.02942711 0.9705729 ]\n",
      " [0.64525104 0.35474896]\n",
      " [0.04920483 0.9507952 ]\n",
      " [0.92713255 0.07286744]\n",
      " [0.59340763 0.4065924 ]\n",
      " [0.49078012 0.5092199 ]\n",
      " [0.95454395 0.04545608]\n",
      " [0.02625597 0.97374403]\n",
      " [0.9432355  0.05676446]\n",
      " [0.6842905  0.31570944]\n",
      " [0.9887484  0.01125165]\n",
      " [0.89624894 0.10375109]\n",
      " [0.1511324  0.8488676 ]\n",
      " [0.7728212  0.22717884]\n",
      " [0.780684   0.21931598]\n",
      " [0.14097285 0.85902715]\n",
      " [0.7916921  0.2083079 ]\n",
      " [0.05285591 0.9471441 ]\n",
      " [0.88897574 0.11102423]\n",
      " [0.08285999 0.91714   ]\n",
      " [0.91199553 0.08800445]\n",
      " [0.07576913 0.9242309 ]\n",
      " [0.8973558  0.10264422]\n",
      " [0.2264868  0.7735132 ]\n",
      " [0.7475696  0.25243038]\n",
      " [0.8973558  0.10264422]\n",
      " [0.18204075 0.81795925]\n",
      " [0.9286414  0.07135862]\n",
      " [0.9019784  0.09802166]\n",
      " [0.81792235 0.18207763]\n",
      " [0.03372324 0.96627676]\n",
      " [0.9399169  0.06008311]\n",
      " [0.94728196 0.05271804]\n",
      " [0.41459292 0.5854071 ]\n",
      " [0.9249607  0.07503933]\n",
      " [0.7388883  0.26111168]\n",
      " [0.6387164  0.3612836 ]\n",
      " [0.05545413 0.94454587]\n",
      " [0.02063143 0.97936857]\n",
      " [0.2264868  0.7735132 ]\n",
      " [0.07064676 0.92935324]\n",
      " [0.541715   0.45828494]\n",
      " [0.94728196 0.05271804]\n",
      " [0.92982966 0.07017034]\n",
      " [0.693197   0.30680296]\n",
      " [0.05778396 0.94221604]\n",
      " [0.91472065 0.08527933]\n",
      " [0.05995607 0.9400439 ]\n",
      " [0.6113499  0.38865012]\n",
      " [0.06109768 0.9389023 ]\n",
      " [0.7675997  0.2324003 ]\n",
      " [0.6449521  0.35504788]\n",
      " [0.95454395 0.04545608]\n",
      " [0.89870983 0.10129018]\n",
      " [0.96860033 0.03139965]\n",
      " [0.89966315 0.10033685]\n",
      " [0.9049501  0.09504994]\n",
      " [0.08067602 0.919324  ]\n",
      " [0.8973558  0.10264422]\n",
      " [0.9286414  0.07135862]\n",
      " [0.9308492  0.06915081]\n",
      " [0.03379726 0.96620274]\n",
      " [0.18442923 0.8155708 ]\n",
      " [0.8578339  0.14216608]\n",
      " [0.94728196 0.05271804]\n",
      " [0.9831841  0.01681589]\n",
      " [0.96860033 0.03139965]\n",
      " [0.80081075 0.19918925]\n",
      " [0.90116173 0.09883825]\n",
      " [0.767374   0.23262602]\n",
      " [0.89966315 0.10033685]\n",
      " [0.09852934 0.90147066]\n",
      " [0.14330435 0.85669565]\n",
      " [0.9106682  0.08933179]\n",
      " [0.04506558 0.9549344 ]\n",
      " [0.8940761  0.10592391]\n",
      " [0.9286414  0.07135862]\n",
      " [0.9143213  0.0856787 ]\n",
      " [0.95454395 0.04545608]\n",
      " [0.76985055 0.23014943]\n",
      " [0.02942711 0.9705729 ]\n",
      " [0.18204075 0.81795925]\n",
      " [0.68824726 0.31175274]\n",
      " [0.14999098 0.850009  ]\n",
      " [0.9823168  0.01768319]\n",
      " [0.96860033 0.03139965]\n",
      " [0.79852676 0.20147325]\n",
      " [0.9106682  0.08933179]\n",
      " [0.88897574 0.11102423]\n",
      " [0.8723055  0.12769449]\n",
      " [0.7330905  0.26690945]\n",
      " [0.9106682  0.08933179]\n",
      " [0.8212471  0.17875293]\n",
      " [0.9019784  0.09802166]\n",
      " [0.92961687 0.07038315]\n",
      " [0.05462611 0.9453739 ]\n",
      " [0.6550797  0.3449203 ]\n",
      " [0.7901155  0.2098845 ]\n",
      " [0.77506816 0.22493187]\n",
      " [0.82266384 0.17733616]\n",
      " [0.7735835  0.22641651]\n",
      " [0.9273795  0.0726205 ]\n",
      " [0.95454395 0.04545608]\n",
      " [0.18204075 0.81795925]\n",
      " [0.14320683 0.85679317]\n",
      " [0.60657847 0.39342153]\n",
      " [0.02530318 0.9746968 ]\n",
      " [0.7292808  0.2707192 ]\n",
      " [0.8732545  0.12674552]\n",
      " [0.90116173 0.09883825]\n",
      " [0.6587808  0.34121916]\n",
      " [0.96860033 0.03139965]\n",
      " [0.7180158  0.28198424]\n",
      " [0.02342016 0.97657984]\n",
      " [0.35716844 0.64283156]\n",
      " [0.6608372  0.33916283]\n",
      " [0.90116173 0.09883825]\n",
      " [0.75117195 0.24882805]\n",
      " [0.9311367  0.06886324]\n",
      " [0.796335   0.203665  ]\n",
      " [0.7979701  0.20202988]\n",
      " [0.9232938  0.07670616]\n",
      " [0.21300751 0.7869925 ]\n",
      " [0.03682333 0.96317667]\n",
      " [0.92897624 0.07102375]\n",
      " [0.03594154 0.96405846]\n",
      " [0.767374   0.23262602]\n",
      " [0.9017497  0.09825031]\n",
      " [0.9574079  0.04259211]\n",
      " [0.04117548 0.9588245 ]\n",
      " [0.6091026  0.39089742]\n",
      " [0.9106682  0.08933179]\n",
      " [0.5408311  0.4591689 ]\n",
      " [0.8260499  0.17395009]\n",
      " [0.71959925 0.28040078]\n",
      " [0.6842905  0.31570944]\n",
      " [0.9621783  0.03782169]\n",
      " [0.8643749  0.13562514]\n",
      " [0.9106682  0.08933179]\n",
      " [0.90116173 0.09883825]\n",
      " [0.92094153 0.07905845]\n",
      " [0.93149936 0.06850064]\n",
      " [0.03019619 0.9698038 ]\n",
      " [0.9116276  0.08837242]\n",
      " [0.37995845 0.62004155]\n",
      " [0.9232938  0.07670616]\n",
      " [0.63492525 0.36507472]\n",
      " [0.9574079  0.04259211]\n",
      " [0.02272511 0.9772749 ]\n",
      " [0.01519889 0.9848011 ]\n",
      " [0.92713255 0.07286744]\n",
      " [0.89624894 0.10375109]\n",
      " [0.91476023 0.08523976]\n",
      " [0.14999098 0.850009  ]\n",
      " [0.8450017  0.15499827]\n",
      " [0.02795064 0.97204936]\n",
      " [0.94728196 0.05271804]\n",
      " [0.89966315 0.10033685]\n",
      " [0.6118547  0.38814533]\n",
      " [0.9740425  0.02595753]\n",
      " [0.01047933 0.98952067]\n",
      " [0.02272511 0.9772749 ]\n",
      " [0.8305024  0.1694976 ]\n",
      " [0.03717631 0.9628237 ]\n",
      " [0.8385221  0.16147791]\n",
      " [0.75045526 0.24954471]\n",
      " [0.82568383 0.1743162 ]\n",
      " [0.01519889 0.9848011 ]\n",
      " [0.79021996 0.20978004]\n",
      " [0.95454395 0.04545608]\n",
      " [0.00696343 0.99303657]\n",
      " [0.96865386 0.03134613]\n",
      " [0.8832857  0.11671428]\n",
      " [0.02697849 0.9730215 ]\n",
      " [0.02191544 0.97808456]\n",
      " [0.87108785 0.12891217]\n",
      " [0.95454395 0.04545608]\n",
      " [0.9106115  0.08938846]\n",
      " [0.9256237  0.07437627]\n",
      " [0.89966315 0.10033685]\n",
      " [0.92961687 0.07038315]\n",
      " [0.58718777 0.4128122 ]\n",
      " [0.7003782  0.2996218 ]\n",
      " [0.8643557  0.13564435]\n",
      " [0.02095824 0.97904176]\n",
      " [0.8386369  0.16136314]\n",
      " [0.89957076 0.10042924]\n",
      " [0.8973558  0.10264422]\n",
      " [0.9518688  0.04813124]\n",
      " [0.6366974  0.36330256]\n",
      " [0.02709103 0.972909  ]\n",
      " [0.86323726 0.13676274]\n",
      " [0.904707   0.09529299]\n",
      " [0.96377134 0.03622868]\n",
      " [0.01107949 0.9889205 ]\n",
      " [0.8818821  0.11811791]\n",
      " [0.0165211  0.9834789 ]\n",
      " [0.8973558  0.10264422]\n",
      " [0.86529195 0.13470806]\n",
      " [0.03818172 0.9618183 ]\n",
      " [0.9286414  0.07135862]\n",
      " [0.00498587 0.99501413]\n",
      " [0.85584766 0.14415234]\n",
      " [0.73524296 0.26475704]\n",
      " [0.62183523 0.37816477]\n",
      " [0.95454395 0.04545608]\n",
      " [0.58356875 0.41643125]\n",
      " [0.25271124 0.74728876]\n",
      " [0.17426473 0.8257353 ]\n",
      " [0.18204075 0.81795925]\n",
      " [0.01038033 0.9896197 ]\n",
      " [0.69938606 0.30061394]\n",
      " [0.88897574 0.11102423]\n",
      " [0.0083015  0.9916985 ]\n",
      " [0.97858083 0.02141916]\n",
      " [0.88897574 0.11102423]\n",
      " [0.5963559  0.40364406]]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict_proba(test_x)[:, 1]\n",
    "print(model.predict_proba(test_x)[:, 1])\n",
    "print(model.predict_proba(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abc7bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = np.where(pred > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2098dd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': pred_label})\n",
    "submission.to_csv('submission_first.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66728850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:37:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:37:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:37:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[00:37:10] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.4123451268684637, 0.4438446446534898, 0.4596008394506547, 0.43782742670946967]\n",
      "[0.8251121076233184, 0.8071748878923767, 0.8116591928251121, 0.8288288288288288]\n",
      "logloss: 0.4384, accuracy:0.8182\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "scores_accuracy = []\n",
    "scores_logloss = []\n",
    "\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "for tr_idx, va_idx in kf.split(train_x):\n",
    "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "    \n",
    "    model = XGBClassifier(n_estimators=20, random_state=71)\n",
    "    model.fit(tr_x, tr_y)\n",
    "    \n",
    "    \n",
    "    va_pred = model.predict_proba(va_x)[:, 1]\n",
    "    \n",
    "    logloss = log_loss(va_y, va_pred)\n",
    "    accuracy = accuracy_score(va_y, va_pred > 0.5)\n",
    "    \n",
    "    scores_logloss.append(logloss)\n",
    "    scores_accuracy.append(accuracy)\n",
    "    \n",
    "print(scores_logloss)\n",
    "print(scores_accuracy)\n",
    "logloss = np.mean(scores_logloss)\n",
    "accuracy = np.mean(scores_accuracy)\n",
    "print(f'logloss: {logloss:.4f}, accuracy:{accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0cbfa9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<itertools.product object at 0x7f885dde3580>\n",
      "max_depth 3 : min_child_weight 1.0\n",
      "max_depth 3 : min_child_weight 2.0\n",
      "max_depth 3 : min_child_weight 4.0\n",
      "max_depth 5 : min_child_weight 1.0\n",
      "max_depth 5 : min_child_weight 2.0\n",
      "max_depth 5 : min_child_weight 4.0\n",
      "max_depth 7 : min_child_weight 1.0\n",
      "max_depth 7 : min_child_weight 2.0\n",
      "max_depth 7 : min_child_weight 4.0\n",
      "[0.4242945477645603, 0.4215542310426872, 0.42260688873757113, 0.4386300601872057, 0.4417016102884996, 0.4290084739885005, 0.4607997813510307, 0.4402184162361671, 0.4361315518237245]\n",
      "1\n",
      "result: max_depth: 3, min_child_weight: 2.0\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "param_space = {\n",
    "    'max_depth': [3,5,7],\n",
    "    'min_child_weight': [1.0,2.0,4.0],\n",
    "}\n",
    "\n",
    "param_combinations = itertools.product(param_space['max_depth'], param_space[\"min_child_weight\"])\n",
    "print(param_combinations)\n",
    "\n",
    "params =[]\n",
    "scores = []\n",
    "\n",
    "for max_depth, min_child_weight in param_combinations:\n",
    "    print(\"max_depth\", max_depth, \": min_child_weight\", min_child_weight)\n",
    "    \n",
    "    score_folds = []\n",
    "    \n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=123456)\n",
    "    for tr_idx, va_idx in kf.split(train_x):\n",
    "        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "        \n",
    "        model = XGBClassifier(n_estimators=20, random_state=71, max_depth=max_depth, min_child_weight=min_child_weight, eval_metric='mlogloss')\n",
    "        model.fit(tr_x, tr_y)\n",
    "        \n",
    "        va_pred = model.predict_proba(va_x)[:, 1]\n",
    "        logloss = log_loss(va_y, va_pred)\n",
    "        score_folds.append(logloss)\n",
    "    \n",
    "    score_mean = np.mean(score_folds)\n",
    "    \n",
    "    params.append((max_depth, min_child_weight))\n",
    "    scores.append(score_mean)\n",
    "\n",
    "print(scores)\n",
    "best_idx = np.argsort(scores)[0]\n",
    "print(best_idx)\n",
    "best_param = params[best_idx]\n",
    "print(f'result: max_depth: {best_param[0]}, min_child_weight: {best_param[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e0ab6572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 5 8 3 7 4 6]\n"
     ]
    }
   ],
   "source": [
    "print(np.argsort(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cbe5877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 元データをコピーする\n",
    "train_x2 = train.drop(['Survived'], axis=1)\n",
    "test_x2 = test.copy()\n",
    "\n",
    "# 変数PassengerIdを除外する\n",
    "train_x2 = train_x2.drop(['PassengerId'], axis=1)\n",
    "test_x2 = test_x2.drop(['PassengerId'], axis=1)\n",
    "\n",
    "# 変数Name, Ticket, Cabinを除外する\n",
    "train_x2 = train_x2.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
    "test_x2 = test_x2.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "# one-hot encodingを行う\n",
    "cat_cols = ['Sex', 'Embarked', 'Pclass']\n",
    "ohe = OneHotEncoder(categories='auto', sparse=False)\n",
    "ohe.fit(train_x2[cat_cols].fillna('NA'))\n",
    "\n",
    "# one-hot encodingのダミー変数の列名を作成する\n",
    "ohe_columns = []\n",
    "for i, c in enumerate(cat_cols):\n",
    "    ohe_columns += [f'{c}_{v}' for v in ohe.categories_[i]]\n",
    "\n",
    "# one-hot encodingによる変換を行う\n",
    "ohe_train_x2 = pd.DataFrame(ohe.transform(train_x2[cat_cols].fillna('NA')), columns=ohe_columns)\n",
    "ohe_test_x2 = pd.DataFrame(ohe.transform(test_x2[cat_cols].fillna('NA')), columns=ohe_columns)\n",
    "\n",
    "# one-hot encoding済みの変数を除外する\n",
    "train_x2 = train_x2.drop(cat_cols, axis=1)\n",
    "test_x2 = test_x2.drop(cat_cols, axis=1)\n",
    "\n",
    "# one-hot encodingで変換された変数を結合する\n",
    "train_x2 = pd.concat([train_x2, ohe_train_x2], axis=1)\n",
    "test_x2 = pd.concat([test_x2, ohe_test_x2], axis=1)\n",
    "\n",
    "# 数値変数の欠損値を学習データの平均で埋める\n",
    "num_cols = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "for col in num_cols:\n",
    "    train_x2[col].fillna(train_x2[col].mean(), inplace=True)\n",
    "    test_x2[col].fillna(train_x2[col].mean(), inplace=True)\n",
    "\n",
    "# 変数Fareを対数変換する\n",
    "train_x2['Fare'] = np.log1p(train_x2['Fare'])\n",
    "test_x2['Fare'] = np.log1p(test_x2['Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6deeffbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:09:26] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 0\n",
      " 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0\n",
      " 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 1 1 1 1 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_xgb = XGBClassifier(n_estimators=20, random_state=71)\n",
    "model_xgb.fit(train_x, train_y)\n",
    "pred_xgb = model_xgb.predict_proba(test_x)[:, 1]\n",
    "\n",
    "model_lr = LogisticRegression(solver='lbfgs', max_iter=300)\n",
    "model_lr.fit(train_x2, train_y)\n",
    "pred_lr = model_lr.predict_proba(test_x2)[:, 1]\n",
    "\n",
    "pred = pred_xgb * 0.8 + pred_lr * 0.2\n",
    "pred_label = np.where(pred > 0.5, 1, 0)\n",
    "print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216fd977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
